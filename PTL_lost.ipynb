{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PTL.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Configurations"],"metadata":{"id":"Ujgk3WYCSqbA"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/IW2_LIACC"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J3XoViqbTrAD","executionInfo":{"status":"ok","timestamp":1653501279347,"user_tz":-60,"elapsed":1994,"user":{"displayName":"Mariana Calado","userId":"05234448846054400290"}},"outputId":"2a2d0b2f-0a41-475e-a6bf-3766cca13058"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/IW2_LIACC\n"]}]},{"cell_type":"code","source":["pip install joblib"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jM4eDClPYa03","executionInfo":{"status":"ok","timestamp":1653498580533,"user_tz":-60,"elapsed":4685,"user":{"displayName":"Erfan Jalili","userId":"13426899455734115468"}},"outputId":"fc69c007-a118-4d00-d5fd-3a7dc7eec80e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (1.1.0)\n"]}]},{"cell_type":"code","source":["pip install emd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xm_QWtdjUtvM","executionInfo":{"status":"ok","timestamp":1653500748996,"user_tz":-60,"elapsed":11187,"user":{"displayName":"Erfan Jalili","userId":"13426899455734115468"}},"outputId":"cd67fc72-7b99-4888-8eed-eb09b9e8a19e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting emd\n","  Downloading emd-0.5.4-py2.py3-none-any.whl (83 kB)\n","\u001b[K     |████████████████████████████████| 83 kB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from emd) (1.1.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from emd) (3.2.2)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from emd) (0.8.9)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from emd) (1.4.1)\n","Collecting dcor\n","  Downloading dcor-0.5.3-py2.py3-none-any.whl (35 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from emd) (1.3.5)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 10.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy<1.22 in /usr/local/lib/python3.7/dist-packages (from emd) (1.21.6)\n","Collecting sparse\n","  Downloading sparse-0.13.0-py2.py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 6.1 MB/s \n","\u001b[?25hRequirement already satisfied: numba>=0.51 in /usr/local/lib/python3.7/dist-packages (from dcor->emd) (0.51.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.51->dcor->emd) (57.4.0)\n","Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.51->dcor->emd) (0.34.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->emd) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->emd) (1.4.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->emd) (2.8.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->emd) (3.0.9)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->emd) (4.2.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->emd) (1.15.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->emd) (2022.1)\n","Installing collected packages: sparse, pyyaml, dcor, emd\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed dcor-0.5.3 emd-0.5.4 pyyaml-6.0 sparse-0.13.0\n"]}]},{"cell_type":"code","source":["pip show emd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RsNX0_vkUj5M","executionInfo":{"status":"ok","timestamp":1653500726221,"user_tz":-60,"elapsed":1153,"user":{"displayName":"Erfan Jalili","userId":"13426899455734115468"}},"outputId":"fdd744a1-cbb3-49cf-a348-6f377348bd17"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Package(s) not found: emd\u001b[0m\n"]}]},{"cell_type":"markdown","source":["# `filter_import File`: Data pre-processing \n","\n","- function of butter_bandpass of the wave to remove noise;\n","- function of normalization\n","- function of Frequency Domain Features: Power Features - Weltch method algorithm\n","- function of Frequency Domain Features: Power Features - Weltch method algorithm norm\n","- functions preprocessing_data that apply a bandpass filter on the raw eeg and on the eeg that their frequencies were divided into alpha, beta, theta, delta and gamma and then the function with normalization"],"metadata":{"id":"Lfb8vfqf61XS"}},{"cell_type":"code","source":["from scipy.signal import butter, lfilter\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.signal import freqz\n","import pandas as pd\n","import csv\n","import statistics \n","from scipy import signal\n","from datetime import datetime\n","from math import factorial\n","from sklearn import preprocessing"],"metadata":{"id":"QxvoTA6o60S-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#butter_bandpass to remove noise\n","def butter_bandpass(lowcut, highcut, fs, order=5):\n","    nyq = 0.5 * fs\n","    low = lowcut / nyq\n","    high = highcut / nyq\n","    b, a = butter(order, [low, high], btype='band')\n","    return b, a\n","\n","def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n","    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n","    y = lfilter(b, a, data)\n","    return y\n","\n","# Normalization\n","def feature_normalize(dataset): \n","    return (dataset - min(dataset))/(max(dataset) - min(dataset))\n","\n","# Frequency Domain Features (Spectral energy, Power Spectral entropy, Spectral centroid, \n","                            # Principal frequency, Fast Fourier Transform) - Power Features - Weltch method algorithm\n","def frequency_domain_features (raw_fil_data, fs):\n","    # Pxx_den - Power Spectral Density & f-frequency\n","    \n","    f, PSD_den_lalpha = signal.welch(raw_fil_data['low-alpha'], fs) \n","    f, PSD_den_halpha = signal.welch(raw_fil_data['high-alpha'], fs)\n","    f, PSD_den_lbeta = signal.welch(raw_fil_data['low-beta'], fs)\n","    f, PSD_den_hbeta = signal.welch(raw_fil_data['high-beta'], fs)\n","    f, PSD_den_Raw = signal.welch(raw_fil_data['rawEEG'], fs) \n","\n","    freq_features ={\n","        'f': f,\n","        'PSD_Raw' : PSD_den_Raw,\n","        'PSD_Lalpha': PSD_den_lalpha,\n","        'PSD_Halpha': PSD_den_halpha,\n","        'PSD_Lbeta': PSD_den_lbeta,\n","        'PSD_Hbeta': PSD_den_hbeta,\n","        }\n","\n","    return freq_features\n","\n","def frequency_domain_features_norm (raw_fil_data, fs): #frequency_domain_features with normalization\n","    # Pxx_den - Power Spectral Density & f-frequency\n","    \n","    f, PSD_den_lalpha = signal.welch(feature_normalize(raw_fil_data['low-alpha']), fs) \n","    f, PSD_den_halpha = signal.welch(feature_normalize(raw_fil_data['high-alpha']), fs)\n","    f, PSD_den_lbeta = signal.welch(feature_normalize(raw_fil_data['low-beta']), fs)\n","    f, PSD_den_hbeta = signal.welch(feature_normalize(raw_fil_data['high-beta']), fs)\n","    f, PSD_den_Raw = signal.welch(feature_normalize(raw_fil_data['rawEEG']), fs) \n","\n","    freq_features_norm ={\n","        'f': f,\n","        'PSD_Raw' : PSD_den_Raw,\n","        'PSD_Lalpha': PSD_den_lalpha,\n","        'PSD_Halpha': PSD_den_halpha,\n","        'PSD_Lbeta': PSD_den_lbeta,\n","        'PSD_Hbeta': PSD_den_hbeta,\n","        }\n","\n","    return freq_features_norm \n","\n","def preprocessing_data(raw_data, fs, order=5): # Apply a bandpass filter on the waves separated by the BCI \n","    lalpha = butter_bandpass_filter(raw_data['low-alpha'], 8, 10, fs, order=order)\n","    halpha = butter_bandpass_filter(raw_data['high-alpha'], 10, 12, fs, order=order)\n","    lbeta = butter_bandpass_filter(raw_data['low-beta'], 12, 18, fs, order=order)\n","    hbeta = butter_bandpass_filter(raw_data['high-beta'], 18, 30, fs, order=order)\n","    raw_fil = butter_bandpass_filter(raw_data['rawEEG'], 4, 45, fs, order=order) \n","    lgamma = butter_bandpass_filter(raw_data['low-gamma'], 30, 50, fs, order=order)\n","    mgamma = butter_bandpass_filter(raw_data['mid-gamma'], 30, 50, fs, order=order)\n","\n","    data_fil ={\n","        'time': raw_data['time'],\n","        'rawEEG' : raw_fil,\n","        'Blink': raw_data['Blink'],\n","        'Attention': raw_data['Attention'], \n","        'Meditation': raw_data['Meditation'],\n","        'delta': raw_data['delta'],\n","        'high-alpha': halpha, \n","        'high-beta': hbeta, \n","        'low-alpha': lalpha, \n","        'low-beta': lbeta, \n","        'low-gamma': raw_data['low-gamma'],\n","        'mid-gamma' : raw_data['mid-gamma'],\n","        'theta' : raw_data['theta'],\n","        }\n","\n","    return data_fil\n","\n","def preprocessing_data_raw(raw_data, fs, order=5):  # Apply a bandpass filter on the raw eeg\n","    lalpha = butter_bandpass_filter(raw_data['rawEEG'], 8, 10, fs, order=order)\n","    halpha = butter_bandpass_filter(raw_data['rawEEG'], 10, 12, fs, order=order)\n","    lbeta = butter_bandpass_filter(raw_data['rawEEG'], 12, 18, fs, order=order)\n","    hbeta = butter_bandpass_filter(raw_data['rawEEG'], 18, 30, fs, order=order)\n","    raw_fil = butter_bandpass_filter(raw_data['rawEEG'], 4, 45, fs, order=order) \n","\n","    data_fil ={\n","        'time': raw_data['time'],\n","        'rawEEG' :raw_fil,\n","        'Blink': raw_data['Blink'],\n","        'Attention': raw_data['Attention'], \n","        'Meditation': raw_data['Meditation'],\n","        'delta': raw_data['delta'],\n","        'high-alpha': halpha, \n","        'high-beta': hbeta, \n","        'low-alpha': lalpha, \n","        'low-beta': lbeta, \n","        'low-gamma': raw_data['low-gamma'],\n","        'mid-gamma' : raw_data['mid-gamma'],\n","        'theta' : raw_data['theta'],\n","        }\n","\n","    return data_fil\n","\n","def preprocessing_data_raw_norm(raw_data, fs, order=5): # Apply a bandpass filter with normalization\n","    lalpha = butter_bandpass_filter(raw_data['rawEEG'], 8, 10, fs, order=order)\n","    halpha = butter_bandpass_filter(raw_data['rawEEG'], 10, 12, fs, order=order)\n","    lbeta = butter_bandpass_filter(raw_data['rawEEG'], 12, 18, fs, order=order)\n","    hbeta = butter_bandpass_filter(raw_data['rawEEG'], 18, 30, fs, order=order)\n","    raw_fil = butter_bandpass_filter(raw_data['rawEEG'], 4, 45, fs, order=order) \n","\n","    data_fil ={\n","        'time': raw_data['time'],\n","        'rawEEG' : feature_normalize(raw_fil),\n","        'Blink': raw_data['Blink'],\n","        'Attention': raw_data['Attention'], \n","        'Meditation': raw_data['Meditation'],\n","        'delta': feature_normalize(raw_data['delta']),\n","        'high-alpha': feature_normalize(halpha), \n","        'high-beta': feature_normalize(hbeta), \n","        'low-alpha': feature_normalize(lalpha), \n","        'low-beta': feature_normalize(lbeta), \n","        'low-gamma': feature_normalize(raw_data['low-gamma']),\n","        'mid-gamma' : feature_normalize(raw_data['mid-gamma']),\n","        'theta' : feature_normalize(raw_data['theta']),\n","        }\n","\n","    return data_fil\n","  "],"metadata":{"id":"DdWqkPwD7UZI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# `utils file:` Helper functions\n","\n","1.   List item\n","2.   List item\n","\n"],"metadata":{"id":"1q8Wa9t3LjHU"}},{"cell_type":"code","source":["import numpy as np\n","from numba import jit\n","from math import log, floor\n","\n","all = ['_embed', '_linear_regression', '_log_n']\n","\n","\n","def _embed(x, order=3, delay=1):\n","    \"\"\"Time-delay embedding.\n","\n","    Parameters\n","    ----------\n","    x : 1d-array\n","        Time series, of shape (n_times)\n","    order : int\n","        Embedding dimension (order).\n","    delay : int\n","        Delay.\n","\n","    Returns\n","    -------\n","    embedded : ndarray\n","        Embedded time-series, of shape (n_times - (order - 1) * delay, order)\n","    \"\"\"\n","    N = len(x)\n","    if order * delay > N:\n","        raise ValueError(\"Error: order * delay should be lower than x.size\")\n","    if delay < 1:\n","        raise ValueError(\"Delay has to be at least 1.\")\n","    if order < 2:\n","        raise ValueError(\"Order has to be at least 2.\")\n","    Y = np.zeros((order, N - (order - 1) * delay))\n","    for i in range(order):\n","        Y[i] = x[(i * delay):(i * delay + Y.shape[1])]\n","    return Y.T\n","\n","\n","@jit('UniTuple(float64, 2)(float64[:], float64[:])', nopython=True)\n","def _linear_regression(x, y):\n","    \"\"\"Fast linear regression using Numba.\n","\n","    Parameters\n","    ----------\n","    x, y : ndarray, shape (n_times,)\n","        Variables\n","\n","    Returns\n","    -------\n","    slope : float\n","        Slope of 1D least-square regression.\n","    intercept : float\n","        Intercept\n","    \"\"\"\n","    n_times = x.size\n","    sx2 = 0\n","    sx = 0\n","    sy = 0\n","    sxy = 0\n","    for j in range(n_times):\n","        sx2 += x[j] ** 2\n","        sx += x[j]\n","        sxy += x[j] * y[j]\n","        sy += y[j]\n","    den = n_times * sx2 - (sx ** 2)\n","    num = n_times * sxy - sx * sy\n","    slope = num / den\n","    intercept = np.mean(y) - slope * np.mean(x)\n","    return slope, intercept\n","\n","\n","@jit('i8[:](f8, f8, f8)', nopython=True)\n","def _log_n(min_n, max_n, factor):\n","    \"\"\"\n","    Creates a list of integer values by successively multiplying a minimum\n","    value min_n by a factor > 1 until a maximum value max_n is reached.\n","\n","    Used for detrended fluctuation analysis (DFA).\n","\n","    Function taken from the nolds python package\n","    (https://github.com/CSchoel/nolds) by Christopher Scholzel.\n","\n","    Parameters\n","    ----------\n","    min_n (float):\n","        minimum value (must be < max_n)\n","    max_n (float):\n","        maximum value (must be > min_n)\n","    factor (float):\n","       factor used to increase min_n (must be > 1)\n","\n","    Returns\n","    -------\n","    list of integers:\n","        min_n, min_n * factor, min_n * factor^2, ... min_n * factor^i < max_n\n","        without duplicates\n","    \"\"\"\n","    max_i = int(floor(log(1.0 * max_n / min_n) / log(factor)))\n","    ns = [min_n]\n","    for i in range(max_i + 1):\n","        n = int(floor(min_n * (factor ** i)))\n","        if n > ns[-1]:\n","            ns.append(n)\n","    return np.array(ns, dtype=np.int64)\n"],"metadata":{"id":"fKTHdcVULthT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## `entpy_entropy file`: Entropy functions\n"],"metadata":{"id":"2KQdflA6IH6y"}},{"cell_type":"code","source":["import numpy as np\n","from numba import jit\n","from math import factorial, log\n","from sklearn.neighbors import KDTree\n","from scipy.signal import periodogram, welch\n","\n","from .utils import _embed\n","\n","all = ['perm_entropy', 'spectral_entropy', 'svd_entropy', 'app_entropy',\n","       'sample_entropy', 'lziv_complexity']\n","\n","\n","def perm_entropy(x, order=3, delay=1, normalize=False):\n","    \"\"\"Permutation Entropy.\n","\n","    Parameters\n","    ----------\n","    x : list or np.array\n","        One-dimensional time series of shape (n_times)\n","    order : int\n","        Order of permutation entropy. Default is 3.\n","    delay : int\n","        Time delay (lag). Default is 1.\n","    normalize : bool\n","        If True, divide by log2(order!) to normalize the entropy between 0\n","        and 1. Otherwise, return the permutation entropy in bit.\n","\n","    Returns\n","    -------\n","    pe : float\n","        Permutation Entropy.\n","\n","    Notes\n","    -----\n","    The permutation entropy is a complexity measure for time-series first\n","    introduced by Bandt and Pompe in 2002.\n","\n","    The permutation entropy of a signal :math:`x` is defined as:\n","\n","    .. math:: H = -\\\\sum p(\\\\pi)\\\\log_2(\\\\pi)\n","\n","    where the sum runs over all :math:`n!` permutations :math:`\\\\pi` of order\n","    :math:`n`. This is the information contained in comparing :math:`n`\n","    consecutive values of the time series. It is clear that\n","    :math:`0 ≤ H (n) ≤ \\\\log_2(n!)` where the lower bound is attained for an\n","    increasing or decreasing sequence of values, and the upper bound for a\n","    completely random system where all :math:`n!` possible permutations appear\n","    with the same probability.\n","\n","    The embedded matrix :math:`Y` is created by:\n","\n","    .. math::\n","        y(i)=[x_i,x_{i+\\\\text{delay}}, ...,x_{i+(\\\\text{order}-1) *\n","        \\\\text{delay}}]\n","\n","    .. math:: Y=[y(1),y(2),...,y(N-(\\\\text{order}-1))*\\\\text{delay})]^T\n","\n","    References\n","    ----------\n","    Bandt, Christoph, and Bernd Pompe. \"Permutation entropy: a\n","    natural complexity measure for time series.\" Physical review letters\n","    88.17 (2002): 174102.\n","\n","    Examples\n","    --------\n","    Permutation entropy with order 2\n","\n","    >>> import numpy as np\n","    >>> import entropy as ent\n","    >>> import stochastic.processes.noise as sn\n","    >>> x = [4, 7, 9, 10, 6, 11, 3]\n","    >>> # Return a value in bit between 0 and log2(factorial(order))\n","    >>> print(f\"{ent.perm_entropy(x, order=2):.4f}\")\n","    0.9183\n","\n","    Normalized permutation entropy with order 3\n","\n","    >>> # Return a value comprised between 0 and 1.\n","    >>> print(f\"{ent.perm_entropy(x, normalize=True):.4f}\")\n","    0.5888\n","\n","    Fractional Gaussian noise with H = 0.5\n","\n","    >>> rng = np.random.default_rng(seed=42)\n","    >>> x = sn.FractionalGaussianNoise(hurst=0.5, rng=rng).sample(10000)\n","    >>> print(f\"{ent.perm_entropy(x, normalize=True):.4f}\")\n","    0.9998\n","\n","    Fractional Gaussian noise with H = 0.9\n","\n","    >>> rng = np.random.default_rng(seed=42)\n","    >>> x = sn.FractionalGaussianNoise(hurst=0.9, rng=rng).sample(10000)\n","    >>> print(f\"{ent.perm_entropy(x, normalize=True):.4f}\")\n","    0.9926\n","\n","    Fractional Gaussian noise with H = 0.1\n","\n","    >>> rng = np.random.default_rng(seed=42)\n","    >>> x = sn.FractionalGaussianNoise(hurst=0.1, rng=rng).sample(10000)\n","    >>> print(f\"{ent.perm_entropy(x, normalize=True):.4f}\")\n","    0.9959\n","\n","    Random\n","\n","    >>> rng = np.random.default_rng(seed=42)\n","    >>> print(f\"{ent.perm_entropy(rng.random(1000), normalize=True):.4f}\")\n","    0.9997\n","\n","    Pure sine wave\n","\n","    >>> x = np.sin(2 * np.pi * 1 * np.arange(3000) / 100)\n","    >>> print(f\"{ent.perm_entropy(x, normalize=True):.4f}\")\n","    0.4463\n","\n","    Linearly-increasing time-series\n","\n","    >>> x = np.arange(1000)\n","    >>> print(f\"{ent.perm_entropy(x, normalize=True):.4f}\")\n","    -0.0000\n","    \"\"\"\n","    x = np.array(x)\n","    ran_order = range(order)\n","    hashmult = np.power(order, ran_order)\n","    # Embed x and sort the order of permutations\n","    sorted_idx = _embed(x, order=order, delay=delay).argsort(kind='quicksort')\n","    # Associate unique integer to each permutations\n","    hashval = (np.multiply(sorted_idx, hashmult)).sum(1)\n","    # Return the counts\n","    _, c = np.unique(hashval, return_counts=True)\n","    # Use np.true_divide for Python 2 compatibility\n","    p = np.true_divide(c, c.sum())\n","    pe = -np.multiply(p, np.log2(p)).sum()\n","    if normalize:\n","        pe /= np.log2(factorial(order))\n","    return pe\n","\n","\n","def spectral_entropy(x, sf, method='fft', nperseg=None, normalize=False):\n","    \"\"\"Spectral Entropy.\n","\n","    Parameters\n","    ----------\n","    x : list or np.array\n","        One-dimensional time series of shape (n_times)\n","    sf : float\n","        Sampling frequency, in Hz.\n","    method : str\n","        Spectral estimation method:\n","\n","        * ``'fft'`` : Fourier Transform (:py:func:`scipy.signal.periodogram`)\n","        * ``'welch'`` : Welch periodogram (:py:func:`scipy.signal.welch`)\n","    nperseg : int or None\n","        Length of each FFT segment for Welch method.\n","        If None (default), uses scipy default of 256 samples.\n","    normalize : bool\n","        If True, divide by log2(psd.size) to normalize the spectral entropy\n","        between 0 and 1. Otherwise, return the spectral entropy in bit.\n","\n","    Returns\n","    -------\n","    se : float\n","        Spectral Entropy\n","\n","    Notes\n","    -----\n","    Spectral Entropy is defined to be the Shannon entropy of the power\n","    spectral density (PSD) of the data:\n","\n","    .. math:: H(x, sf) =  -\\\\sum_{f=0}^{f_s/2} P(f) \\\\log_2[P(f)]\n","\n","    Where :math:`P` is the normalised PSD, and :math:`f_s` is the sampling\n","    frequency.\n","\n","    References\n","    ----------\n","    Inouye, T. et al. (1991). Quantification of EEG irregularity by\n","    use of the entropy of the power spectrum. Electroencephalography\n","    and clinical neurophysiology, 79(3), 204-210.\n","\n","    https://en.wikipedia.org/wiki/Spectral_density\n","\n","    https://en.wikipedia.org/wiki/Welch%27s_method\n","\n","    Examples\n","    --------\n","    Spectral entropy of a pure sine using FFT\n","\n","    >>> import numpy as np\n","    >>> import entropy as ent\n","    >>> sf, f, dur = 100, 1, 4\n","    >>> N = sf * dur # Total number of discrete samples\n","    >>> t = np.arange(N) / sf # Time vector\n","    >>> x = np.sin(2 * np.pi * f * t)\n","    >>> np.round(ent.spectral_entropy(x, sf, method='fft'), 2)\n","    0.0\n","\n","    Spectral entropy of a random signal using Welch's method\n","\n","    >>> np.random.seed(42)\n","    >>> x = np.random.rand(3000)\n","    >>> ent.spectral_entropy(x, sf=100, method='welch')\n","    6.980045662371389\n","\n","    Normalized spectral entropy\n","\n","    >>> ent.spectral_entropy(x, sf=100, method='welch', normalize=True)\n","    0.9955526198316071\n","\n","    Fractional Gaussian noise with H = 0.5\n","\n","    >>> import stochastic.processes.noise as sn\n","    >>> rng = np.random.default_rng(seed=42)\n","    >>> x = sn.FractionalGaussianNoise(hurst=0.5, rng=rng).sample(10000)\n","    >>> print(f\"{ent.spectral_entropy(x, sf=100, normalize=True):.4f}\")\n","    0.9505\n","\n","    Fractional Gaussian noise with H = 0.9\n","\n","    >>> rng = np.random.default_rng(seed=42)\n","    >>> x = sn.FractionalGaussianNoise(hurst=0.9, rng=rng).sample(10000)\n","    >>> print(f\"{ent.spectral_entropy(x, sf=100, normalize=True):.4f}\")\n","    0.8477\n","\n","    Fractional Gaussian noise with H = 0.1\n","\n","    >>> rng = np.random.default_rng(seed=42)\n","    >>> x = sn.FractionalGaussianNoise(hurst=0.1, rng=rng).sample(10000)\n","    >>> print(f\"{ent.spectral_entropy(x, sf=100, normalize=True):.4f}\")\n","    0.9248\n","    \"\"\"\n","    x = np.array(x)\n","    # Compute and normalize power spectrum\n","    if method == 'fft':\n","        _, psd = periodogram(x, sf)\n","    elif method == 'welch':\n","        _, psd = welch(x, sf, nperseg=nperseg)\n","    psd_norm = np.divide(psd, psd.sum())\n","    se = -np.multiply(psd_norm, np.log2(psd_norm)).sum()\n","    if normalize:\n","        se /= np.log2(psd_norm.size)\n","    return se\n","\n","\n","def svd_entropy(x, order=3, delay=1, normalize=False):\n","    \"\"\"Singular Value Decomposition entropy.\n","\n","    Parameters\n","    ----------\n","    x : list or np.array\n","        One-dimensional time series of shape (n_times)\n","    order : int\n","        Order of SVD entropy (= length of the embedding dimension).\n","        Default is 3.\n","    delay : int\n","        Time delay (lag). Default is 1.\n","    normalize : bool\n","        If True, divide by log2(order!) to normalize the entropy between 0\n","        and 1. Otherwise, return the permutation entropy in bit.\n","\n","    Returns\n","    -------\n","    svd_e : float\n","        SVD Entropy\n","\n","    Notes\n","    -----\n","    SVD entropy is an indicator of the number of eigenvectors that are needed\n","    for an adequate explanation of the data set. In other words, it measures\n","    the dimensionality of the data.\n","\n","    The SVD entropy of a signal :math:`x` is defined as:\n","\n","    .. math::\n","        H = -\\\\sum_{i=1}^{M} \\\\overline{\\\\sigma}_i log_2(\\\\overline{\\\\sigma}_i)\n","\n","    where :math:`M` is the number of singular values of the embedded matrix\n","    :math:`Y` and :math:`\\\\sigma_1, \\\\sigma_2, ..., \\\\sigma_M` are the\n","    normalized singular values of :math:`Y`.\n","\n","    The embedded matrix :math:`Y` is created by:\n","\n","    .. math::\n","        y(i)=[x_i,x_{i+\\\\text{delay}}, ...,x_{i+(\\\\text{order}-1) *\n","        \\\\text{delay}}]\n","\n","    .. math:: Y=[y(1),y(2),...,y(N-(\\\\text{order}-1))*\\\\text{delay})]^T\n","\n","    Examples\n","    --------\n","    SVD entropy with order 2\n","\n","    >>> import numpy as np\n","    >>> import entropy as ent\n","    >>> import stochastic.processes.noise as sn\n","    >>> x = [4, 7, 9, 10, 6, 11, 3]\n","    >>> # Return a value in bit between 0 and log2(factorial(order))\n","    >>> print(ent.svd_entropy(x, order=2))\n","    0.7618909465130066\n","\n","    Normalized SVD entropy with order 3\n","\n","    >>> x = [4, 7, 9, 10, 6, 11, 3]\n","    >>> # Return a value comprised between 0 and 1.\n","    >>> print(ent.svd_entropy(x, order=3, normalize=True))\n","    0.6870083043946692\n","\n","    Fractional Gaussian noise with H = 0.5\n","\n","    >>> rng = np.random.default_rng(seed=42)\n","    >>> x = sn.FractionalGaussianNoise(hurst=0.5, rng=rng).sample(10000)\n","    >>> print(f\"{ent.svd_entropy(x, normalize=True):.4f}\")\n","    1.0000\n","\n","    Fractional Gaussian noise with H = 0.9\n","\n","    >>> rng = np.random.default_rng(seed=42)\n","    >>> x = sn.FractionalGaussianNoise(hurst=0.9, rng=rng).sample(10000)\n","    >>> print(f\"{ent.svd_entropy(x, normalize=True):.4f}\")\n","    0.9080\n","\n","    Fractional Gaussian noise with H = 0.1\n","\n","    >>> rng = np.random.default_rng(seed=42)\n","    >>> x = sn.FractionalGaussianNoise(hurst=0.1, rng=rng).sample(10000)\n","    >>> print(f\"{ent.svd_entropy(x, normalize=True):.4f}\")\n","    0.9637\n","\n","    Random\n","\n","    >>> rng = np.random.default_rng(seed=42)\n","    >>> print(f\"{ent.svd_entropy(rng.random(1000), normalize=True):.4f}\")\n","    0.8527\n","\n","    Pure sine wave\n","\n","    >>> x = np.sin(2 * np.pi * 1 * np.arange(3000) / 100)\n","    >>> print(f\"{ent.svd_entropy(x, normalize=True):.4f}\")\n","    0.1775\n","\n","    Linearly-increasing time-series\n","\n","    >>> x = np.arange(1000)\n","    >>> print(f\"{ent.svd_entropy(x, normalize=True):.4f}\")\n","    0.0053\n","    \"\"\"\n","    x = np.array(x)\n","    mat = _embed(x, order=order, delay=delay)\n","    W = np.linalg.svd(mat, compute_uv=False)\n","    # Normalize the singular values\n","    W /= sum(W)\n","    svd_e = -np.multiply(W, np.log2(W)).sum()\n","    if normalize:\n","        svd_e /= np.log2(order)\n","    return svd_e\n","\n","\n","def _app_samp_entropy(x, order, metric='chebyshev', approximate=True):\n","    \"\"\"Utility function for `app_entropy`` and `sample_entropy`.\n","    \"\"\"\n","    _all_metrics = KDTree.valid_metrics\n","    if metric not in _all_metrics:\n","        raise ValueError('The given metric (%s) is not valid. The valid '\n","                         'metric names are: %s' % (metric, _all_metrics))\n","    phi = np.zeros(2)\n","    r = 0.2 * np.std(x, ddof=0)\n","\n","    # compute phi(order, r)\n","    _emb_data1 = _embed(x, order, 1)\n","    if approximate:\n","        emb_data1 = _emb_data1\n","    else:\n","        emb_data1 = _emb_data1[:-1]\n","    count1 = KDTree(emb_data1, metric=metric).query_radius(emb_data1, r,\n","                                                           count_only=True\n","                                                           ).astype(np.float64)\n","    # compute phi(order + 1, r)\n","    emb_data2 = _embed(x, order + 1, 1)\n","    count2 = KDTree(emb_data2, metric=metric).query_radius(emb_data2, r,\n","                                                           count_only=True\n","                                                           ).astype(np.float64)\n","    if approximate:\n","        phi[0] = np.mean(np.log(count1 / emb_data1.shape[0]))\n","        phi[1] = np.mean(np.log(count2 / emb_data2.shape[0]))\n","    else:\n","        phi[0] = np.mean((count1 - 1) / (emb_data1.shape[0] - 1))\n","        phi[1] = np.mean((count2 - 1) / (emb_data2.shape[0] - 1))\n","    return phi\n","\n","\n","@jit('f8(f8[:], i4, f8)', nopython=True)\n","def _numba_sampen(x, order, r):\n","    \"\"\"\n","    Fast evaluation of the sample entropy using Numba.\n","    \"\"\"\n","    n = x.size\n","    n1 = n - 1\n","    order += 1\n","    order_dbld = 2 * order\n","\n","    # Define threshold\n","    # r *= x.std()\n","\n","    # initialize the lists\n","    run = [0] * n\n","    run1 = run[:]\n","    r1 = [0] * (n * order_dbld)\n","    a = [0] * order\n","    b = a[:]\n","    p = a[:]\n","\n","    for i in range(n1):\n","        nj = n1 - i\n","\n","        for jj in range(nj):\n","            j = jj + i + 1\n","            if abs(x[j] - x[i]) < r:\n","                run[jj] = run1[jj] + 1\n","                m1 = order if order < run[jj] else run[jj]\n","                for m in range(m1):\n","                    a[m] += 1\n","                    if j < n1:\n","                        b[m] += 1\n","            else:\n","                run[jj] = 0\n","        for j in range(order_dbld):\n","            run1[j] = run[j]\n","            r1[i + n * j] = run[j]\n","        if nj > order_dbld - 1:\n","            for j in range(order_dbld, nj):\n","                run1[j] = run[j]\n","\n","    m = order - 1\n","\n","    while m > 0:\n","        b[m] = b[m - 1]\n","        m -= 1\n","\n","    b[0] = n * n1 / 2\n","    a = np.array([float(aa) for aa in a])\n","    b = np.array([float(bb) for bb in b])\n","    p = np.true_divide(a, b)\n","    return -log(p[-1])\n","\n","\n","def app_entropy(x, order=2, metric='chebyshev'):\n","    \"\"\"Approximate Entropy.\n","\n","    Parameters\n","    ----------\n","    x : list or np.array\n","        One-dimensional time series of shape (n_times).\n","    order : int\n","        Embedding dimension. Default is 2.\n","    metric : str\n","        Name of the distance metric function used with\n","        :py:class:`sklearn.neighbors.KDTree`. Default is to use the\n","        `Chebyshev <https://en.wikipedia.org/wiki/Chebyshev_distance>`_\n","        distance.\n","\n","    Returns\n","    -------\n","    ae : float\n","        Approximate Entropy.\n","\n","    Notes\n","    -----\n","    Approximate entropy is a technique used to quantify the amount of\n","    regularity and the unpredictability of fluctuations over time-series data.\n","    Smaller values indicates that the data is more regular and predictable.\n","\n","    The tolerance value (:math:`r`) is set to :math:`0.2 * \\\\text{std}(x)`.\n","\n","    Code adapted from the `mne-features <https://mne.tools/mne-features/>`_\n","    package by Jean-Baptiste Schiratti and Alexandre Gramfort.\n","\n","    References\n","    ----------\n","    Richman, J. S. et al. (2000). Physiological time-series analysis\n","    using approximate entropy and sample entropy. American Journal of\n","    Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.\n","\n","    https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html\n","\n","    Examples\n","    --------\n","    Fractional Gaussian noise with H = 0.5\n","\n","    >>> import numpy as np\n","    >>> import entropy as ent\n","    >>> import stochastic.processes.noise as sn\n","    >>> rng = np.random.default_rng(seed=42)\n","    >>> x = sn.FractionalGaussianNoise(hurst=0.5, rng=rng).sample(10000)\n","    >>> print(f\"{ent.app_entropy(x, order=2):.4f}\")\n","    2.1958\n","\n","    Same with order = 3 and metric = 'euclidean'\n","\n","    >>> print(f\"{ent.app_entropy(x, order=3, metric='euclidean'):.4f}\")\n","    1.5120\n","\n","    Fractional Gaussian noise with H = 0.9\n","\n","    >>> rng = np.random.default_rng(seed=42)\n","    >>> x = sn.FractionalGaussianNoise(hurst=0.9, rng=rng).sample(10000)\n","    >>> print(f\"{ent.app_entropy(x):.4f}\")\n","    1.9681\n","\n","    Fractional Gaussian noise with H = 0.1\n","\n","    >>> rng = np.random.default_rng(seed=42)\n","    >>> x = sn.FractionalGaussianNoise(hurst=0.1, rng=rng).sample(10000)\n","    >>> print(f\"{ent.app_entropy(x):.4f}\")\n","    2.0906\n","\n","    Random\n","\n","    >>> rng = np.random.default_rng(seed=42)\n","    >>> print(f\"{ent.app_entropy(rng.random(1000)):.4f}\")\n","    1.8177\n","\n","    Pure sine wave\n","\n","    >>> x = np.sin(2 * np.pi * 1 * np.arange(3000) / 100)\n","    >>> print(f\"{ent.app_entropy(x):.4f}\")\n","    0.2009\n","\n","    Linearly-increasing time-series\n","\n","    >>> x = np.arange(1000)\n","    >>> print(f\"{ent.app_entropy(x):.4f}\")\n","    -0.0010\n","    \"\"\"\n","    phi = _app_samp_entropy(x, order=order, metric=metric, approximate=True)\n","    return np.subtract(phi[0], phi[1])\n","\n","\n","def sample_entropy(x, order=2, metric='chebyshev'):\n","    \"\"\"Sample Entropy.\n","\n","    Parameters\n","    ----------\n","    x : list or np.array\n","        One-dimensional time series of shape (n_times).\n","    order : int\n","        Embedding dimension. Default is 2.\n","    metric : str\n","        Name of the distance metric function used with\n","        :py:class:`sklearn.neighbors.KDTree`. Default is to use the\n","        `Chebyshev <https://en.wikipedia.org/wiki/Chebyshev_distance>`_\n","        distance.\n","\n","    Returns\n","    -------\n","    se : float\n","        Sample Entropy.\n","\n","    Notes\n","    -----\n","    Sample entropy is a modification of approximate entropy, used for assessing\n","    the complexity of physiological time-series signals. It has two advantages\n","    over approximate entropy: data length independence and a relatively\n","    trouble-free implementation. Large values indicate high complexity whereas\n","    smaller values characterize more self-similar and regular signals.\n","\n","    The sample entropy of a signal :math:`x` is defined as:\n","\n","    .. math:: H(x, m, r) = -\\\\log\\\\frac{C(m + 1, r)}{C(m, r)}\n","\n","    where :math:`m` is the embedding dimension (= order), :math:`r` is\n","    the radius of the neighbourhood (default = :math:`0.2 * \\\\text{std}(x)`),\n","    :math:`C(m + 1, r)` is the number of embedded vectors of length\n","    :math:`m + 1` having a\n","    `Chebyshev distance <https://en.wikipedia.org/wiki/Chebyshev_distance>`_\n","    inferior to :math:`r` and :math:`C(m, r)` is the number of embedded\n","    vectors of length :math:`m` having a Chebyshev distance inferior to\n","    :math:`r`.\n","\n","    Note that if ``metric == 'chebyshev'`` and ``len(x) < 5000`` points,\n","    then the sample entropy is computed using a fast custom Numba script.\n","    For other distance metric or longer time-series, the sample entropy is\n","    computed using a code from the\n","    `mne-features <https://mne.tools/mne-features/>`_ package by Jean-Baptiste\n","    Schiratti and Alexandre Gramfort (requires sklearn).\n","\n","    References\n","    ----------\n","    Richman, J. S. et al. (2000). Physiological time-series analysis\n","    using approximate entropy and sample entropy. American Journal of\n","    Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.\n","\n","    https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html\n","\n","    Examples\n","    --------\n","    Fractional Gaussian noise with H = 0.5\n","\n","    >>> import numpy as np\n","    >>> import entropy as ent\n","    >>> import stochastic.processes.noise as sn\n","    >>> rng = np.random.default_rng(seed=42)\n","    >>> x = sn.FractionalGaussianNoise(hurst=0.5, rng=rng).sample(10000)\n","    >>> print(f\"{ent.sample_entropy(x, order=2):.4f}\")\n","    2.1819\n","\n","    Same with order = 3 and using the Euclidean distance\n","\n","    >>> print(f\"{ent.sample_entropy(x, order=3, metric='euclidean'):.4f}\")\n","    2.6806\n","\n","    Fractional Gaussian noise with H = 0.9\n","\n","    >>> rng = np.random.default_rng(seed=42)\n","    >>> x = sn.FractionalGaussianNoise(hurst=0.9, rng=rng).sample(10000)\n","    >>> print(f\"{ent.sample_entropy(x):.4f}\")\n","    1.9078\n","\n","    Fractional Gaussian noise with H = 0.1\n","\n","    >>> rng = np.random.default_rng(seed=42)\n","    >>> x = sn.FractionalGaussianNoise(hurst=0.1, rng=rng).sample(10000)\n","    >>> print(f\"{ent.sample_entropy(x):.4f}\")\n","    2.0555\n","\n","    Random\n","\n","    >>> rng = np.random.default_rng(seed=42)\n","    >>> print(f\"{ent.sample_entropy(rng.random(1000)):.4f}\")\n","    2.2017\n","\n","    Pure sine wave\n","\n","    >>> x = np.sin(2 * np.pi * 1 * np.arange(3000) / 100)\n","    >>> print(f\"{ent.sample_entropy(x):.4f}\")\n","    0.1633\n","\n","    Linearly-increasing time-series\n","\n","    >>> x = np.arange(1000)\n","    >>> print(f\"{ent.sample_entropy(x):.4f}\")\n","    -0.0000\n","    \"\"\"\n","    x = np.asarray(x, dtype=np.float64)\n","    if metric == 'chebyshev' and x.size < 5000:\n","        return _numba_sampen(x, order=order, r=(0.2 * x.std(ddof=0)))\n","    else:\n","        phi = _app_samp_entropy(x, order=order, metric=metric,\n","                                approximate=False)\n","        return -np.log(np.divide(phi[1], phi[0]))\n","\n","\n","'''@jit('u8(unicode_type)', nopython=True)\n","def _lz_complexity(binary_string):\n","    \"\"\"Internal Numba implementation of the Lempel-Ziv (LZ) complexity.\n","\n","    https://github.com/Naereen/Lempel-Ziv_Complexity/blob/master/src/lziv_complexity.py\n","    \"\"\"\n","    u, v, w = 0, 1, 1\n","    v_max = 1\n","    length = len(binary_string)\n","    complexity = 1\n","    while True:\n","        if binary_string[u + v - 1] == binary_string[w + v - 1]:\n","            v += 1\n","            if w + v >= length:\n","                complexity += 1\n","                break\n","        else:\n","            v_max = max(v, v_max)\n","            u += 1\n","            if u == w:\n","                complexity += 1\n","                w += v_max\n","                if w >= length:\n","                    break\n","                else:\n","                    u = 0\n","                    v = 1\n","                    v_max = 1\n","            else:\n","                v = 1\n","    return complexity'''\n","\n","\n","def lziv_complexity(sequence, normalize=False):\n","    \"\"\"\n","    Lempel-Ziv (LZ) complexity of (binary) sequence.\n","\n","    .. versionadded:: 0.1.1\n","\n","    Parameters\n","    ----------\n","    sequence : str or array\n","        A sequence of character, e.g. ``'1001111011000010'``,\n","        ``[0, 1, 0, 1, 1]``, or ``'Hello World!'``.\n","    normalize : bool\n","        If ``True``, returns the normalized LZ (see Notes).\n","\n","    Returns\n","    -------\n","    lz : int or float\n","        LZ complexity, which corresponds to the number of different\n","        substrings encountered as the stream is viewed from the\n","        beginning to the end. If ``normalize=False``, the output is an\n","        integer (counts), otherwise the output is a float.\n","\n","    Notes\n","    -----\n","    LZ complexity is defined as the number of different substrings encountered\n","    as the sequence is viewed from begining to the end.\n","\n","    Although the raw LZ is an important complexity indicator, it is heavily\n","    influenced by sequence length (longer sequence will result in higher LZ).\n","    Zhang and colleagues (2009) have therefore proposed the normalized LZ,\n","    which is defined by\n","\n","    .. math:: \\\\text{LZn} = \\\\frac{\\\\text{LZ}}{(n / \\\\log_b{n})}\n","\n","    where :math:`n` is the length of the sequence and :math:`b` the number of\n","    unique characters in the sequence.\n","\n","    References\n","    ----------\n","    * Lempel, A., & Ziv, J. (1976). On the Complexity of Finite Sequences.\n","      IEEE Transactions on Information Theory / Professional Technical\n","      Group on Information Theory, 22(1), 75–81.\n","      https://doi.org/10.1109/TIT.1976.1055501\n","\n","    * Zhang, Y., Hao, J., Zhou, C., & Chang, K. (2009). Normalized\n","      Lempel-Ziv complexity and its application in bio-sequence analysis.\n","      Journal of Mathematical Chemistry, 46(4), 1203–1212.\n","      https://doi.org/10.1007/s10910-008-9512-2\n","\n","    * https://en.wikipedia.org/wiki/Lempel-Ziv_complexity\n","\n","    * https://github.com/Naereen/Lempel-Ziv_Complexity\n","\n","    Examples\n","    --------\n","    >>> from entropy import lziv_complexity\n","    >>> # Substrings = 1 / 0 / 01 / 1110 / 1100 / 0010\n","    >>> s = '1001111011000010'\n","    >>> lziv_complexity(s)\n","    6\n","\n","    Using a list of integer / boolean instead of a string:\n","\n","    >>> # 1 / 0 / 10\n","    >>> lziv_complexity([1, 0, 1, 0, 1, 0, 1, 0, 1, 0])\n","    3\n","\n","    With normalization:\n","\n","    >>> lziv_complexity(s, normalize=True)\n","    1.5\n","\n","    This function also works with characters and words:\n","\n","    >>> s = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n","    >>> lziv_complexity(s), lziv_complexity(s, normalize=True)\n","    (26, 1.0)\n","\n","    >>> s = 'HELLO WORLD! HELLO WORLD! HELLO WORLD! HELLO WORLD!'\n","    >>> lziv_complexity(s), lziv_complexity(s, normalize=True)\n","    (11, 0.38596001132145313)\n","    \"\"\"\n","    assert isinstance(sequence, (str, list, np.ndarray))\n","    assert isinstance(normalize, bool)\n","    if isinstance(sequence, (list, np.ndarray)):\n","        sequence = np.asarray(sequence)\n","        if sequence.dtype.kind in 'bfi':\n","            # Convert [True, False] or [1., 0.] to [1, 0]\n","            sequence = sequence.astype(int)\n","        # Convert to a string, e.g. \"10001100\"\n","        s = ''.join(sequence.astype(str))\n","    else:\n","        s = sequence\n","\n","    if normalize:\n","        # 1) Timmermann et al. 2019\n","        # The sequence is randomly shuffled, and the normalized LZ\n","        # is calculated as the ratio of the LZ of the original sequence\n","        # divided by the LZ of the randomly shuffled LZ. However, the final\n","        # output is dependent on the random seed.\n","        # sl_shuffled = list(s)\n","        # rng = np.random.RandomState(None)\n","        # rng.shuffle(sl_shuffled)\n","        # s_shuffled = ''.join(sl_shuffled)\n","        # return _lz_complexity(s) / _lz_complexity(s_shuffled)\n","        # 2) Zhang et al. 2009\n","        n = len(s)\n","        base = len(''.join(set(s)))  # Number of unique characters\n","        base = 2 if base < 2 else base\n","        return _lz_complexity(s) / (n / log(n, base))\n","    else:\n","        return _lz_complexity(s)\n"],"metadata":{"id":"ubxMuh-jIn1n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fractal functions"],"metadata":{"id":"1WEwhg1_MFgT"}},{"cell_type":"markdown","source":["# `# Functions File:` Functions used for data processing, features extration"],"metadata":{"id":"cA4lNaPZF0G6"}},{"cell_type":"code","source":["from entpy.entropy import * \n","from entpy.fractal import * \n","from scipy.stats import kurtosis\n","import numpy as np\n","from Filter_import import *\n","import scipy as sp\n","from scipy.signal import hilbert\n","from scipy import signal\n","import pandas as pd\n","import numpy as np\n","from Filter_import import *\n","import matplotlib.pyplot as plt\n","from pandas import read_csv\n","from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.feature_selection import VarianceThreshold, f_classif, f_regression, SelectKBest, SelectPercentile, chi2, mutual_info_classif\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.svm import LinearSVC\n","import os, re\n","from sklearn import preprocessing\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.externals import joblib\n","from scipy.stats import skew\n","import emd\n","from sklearn.ensemble import AdaBoostClassifier\n","import statistics\n","from scipy.stats import entropy"],"metadata":{"id":"_EhibNoQFzzQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# `ml_use.py file`: Main file\n","\n","\n","\n"," "],"metadata":{"id":"PvD6BAmeHvcz"}},{"cell_type":"code","source":["from pandas import read_csv\n","from functions import *\n","from Filter_import import *\n","from sklearn.model_selection import learning_curve\n","from sklearn.ensemble import VotingClassifier\n","import math\n","import datetime\n","import pandas as pd\n","import re, os\n","from scipy.signal import resample"],"metadata":{"id":"objyrFW7T8QO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Dataset Files\n","dataset = 'DatasetA'\n","i_folder = 11 # user folder\n","\n","#Seconds per epoch\n","epoch_time = 4\n","grid_CV = 5"],"metadata":{"id":"vdClS56NXVwI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Count the number of files of the Dataset\n","path, dirs, files = next(os.walk(dataset))\n","\n","#Remove the Labels file from the list\n","files.remove('Labels.csv')\n","dirs_count = len(dirs)\n","\n","label_file = read_csv(dataset +'\\Labels.csv')\n","#Sort files in ascending order\n","dirs.sort(key=lambda var:[int(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])"],"metadata":{"id":"3IS-19wgUL5g"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nz--4_5GR8Ue"},"outputs":[],"source":["# Dataset Files\n","dataset = 'DatasetA'\n","i_folder = 11 # user folder\n","\n","#Seconds per epoch\n","epoch_time = 4\n","grid_CV = 5"]}]}